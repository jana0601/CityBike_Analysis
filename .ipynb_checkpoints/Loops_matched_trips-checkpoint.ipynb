{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbfde47-90a9-4a05-9f8b-e098ed7a8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "crash_data_path = r\"C:\\Users\\yanhu\\Documents\\Python_Notebook\\AXA_task\\accident_data\\sorted_bike_crashes.csv\"\n",
    "crash_df_chunk = pd.read_csv(crash_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8467a3-d39f-40ea-bb02-a3779cac48cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRASH_DATETIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE', 'LOCATION', 'ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME', 'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED', 'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5', 'COLLISION_ID', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#filepath = r\"C:\\Users\\zhao\\Documents\\workspace\\task\\clean_data_bike\\2013_merged_no_duplicate.csv\"\n",
    "#filepath = r\"C:\\Users\\zhao\\Documents\\workspace\\task\\accident_data\\Motor_Vehicle_Collisions_-_Crashes_20251009.csv\"\n",
    "with open(crash_data_path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    keys = reader.fieldnames\n",
    "    print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d160ba-47be-47fc-8ac2-475e7daecfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from haversine import haversine, Unit\n",
    "from pandas import Timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8237a84-e5b5-4e2f-b8de-4db3abf72e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "trip_data_folder = Path(r\"C:\\Users\\yanhu\\Documents\\Python_Notebook\\AXA_task\\merged_tripdata3\")\n",
    "crash_file = Path(r\"C:\\Users\\yanhu\\Documents\\Python_Notebook\\AXA_task\\accident_data\\sorted_bike_crashes.csv\")  # or your processed crash file\n",
    "results_folder = Path(\"results\\matched_trips\")\n",
    "results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "max_time_diff = 1  # minutes\n",
    "max_distance_m = 100  # meters\n",
    "\n",
    "# === LOAD CRASH DATA ===\n",
    "crash_df = pd.read_csv(crash_file)\n",
    "#crash_df['CRASH_DATETIME'] = pd.to_datetime(crash_df['CRASH_DATE'] + ' ' + crash_df['CRASH_TIME'])\n",
    "\n",
    "# Keep only necessary columns to speed up\n",
    "crash_df = crash_df[['COLLISION_ID', 'CRASH_DATETIME', 'LATITUDE', 'LONGITUDE']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10860881-33a3-4c83-9c32-3f1f7cf55ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_df['CRASH_DATETIME'] = pd.to_datetime(crash_df['CRASH_DATETIME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c506aafd-1a27-4e5b-add5-de227ccf2611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: merged_tripdata3\\2015_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanhu\\AppData\\Local\\Temp\\ipykernel_17660\\2760499234.py:19: DtypeWarning: Columns (16,20,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trip_df = pd.read_csv(trip_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ 3874 crash records within trip time range.\n",
      "âœ… Matched trips found: 469\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2015.csv\n",
      "Processing file: merged_tripdata3\\2016_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanhu\\AppData\\Local\\Temp\\ipykernel_17660\\2760499234.py:19: DtypeWarning: Columns (4,8,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trip_df = pd.read_csv(trip_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ 4280 crash records within trip time range.\n",
      "âœ… Matched trips found: 426\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2016.csv\n",
      "Processing file: merged_tripdata3\\2017_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanhu\\AppData\\Local\\Temp\\ipykernel_17660\\2760499234.py:19: DtypeWarning: Columns (4,8,12,16,20,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trip_df = pd.read_csv(trip_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ 4567 crash records within trip time range.\n",
      "âœ… Matched trips found: 575\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2017.csv\n",
      "Processing file: merged_tripdata3\\2018_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2018\n",
      "ðŸš¨ 4432 crash records within trip time range.\n",
      "âœ… Matched trips found: 490\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2018.csv\n",
      "Processing file: merged_tripdata3\\2019_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2019\n",
      "ðŸš¨ 4696 crash records within trip time range.\n",
      "âœ… Matched trips found: 707\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2019.csv\n",
      "Processing file: merged_tripdata3\\2020_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2020\n",
      "ðŸš¨ 5189 crash records within trip time range.\n",
      "âœ… Matched trips found: 0\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2020.csv\n",
      "Processing file: merged_tripdata3\\2021_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanhu\\AppData\\Local\\Temp\\ipykernel_17660\\2760499234.py:19: DtypeWarning: Columns (4,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trip_df = pd.read_csv(trip_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ 4643 crash records within trip time range.\n",
      "âœ… Matched trips found: 0\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2021.csv\n",
      "Processing file: merged_tripdata3\\2022_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2022\n",
      "ðŸš¨ 4647 crash records within trip time range.\n",
      "âœ… Matched trips found: 0\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2022.csv\n",
      "Processing file: merged_tripdata3\\2023_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2023\n",
      "ðŸš¨ 4872 crash records within trip time range.\n",
      "âœ… Matched trips found: 0\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2023.csv\n",
      "Processing file: merged_tripdata3\\2024_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2024\n",
      "ðŸš¨ 4711 crash records within trip time range.\n",
      "âœ… Matched trips found: 1292\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2024.csv\n",
      "Processing file: merged_tripdata3\\2025_merged.csv\n",
      "\n",
      "ðŸ“¦ Processing trips for year: 2025\n",
      "ðŸš¨ 3949 crash records within trip time range.\n",
      "âœ… Matched trips found: 1158\n",
      "ðŸ’¾ Saved to: results\\matched_trips\\matched_trips_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# === LOOP OVER TRIP FILES ===\n",
    "#for trip_file in trip_data_folder.glob(\"*.csv\"):\n",
    "    #year = trip_file.stem.split('_')[0]  # e.g., \"2015\" from \"2015_merged.csv\"\n",
    "#for trip_file in [\n",
    "#    r'merged_tripdata3\\2013_merged.csv',\n",
    "#    r'merged_tripdata3\\2014_merged.csv'\n",
    "#]:\n",
    "\n",
    "for year in range(2015, 2026):  # 2025 inclusive\n",
    "    trip_file = fr'merged_tripdata3\\{year}_merged.csv'\n",
    "    print(f\"Processing file: {trip_file}\")\n",
    "    \n",
    "    filename = os.path.basename(trip_file)  # get \"2022_merged.csv\"\n",
    "    year = filename.split('_')[0]            # get \"2022\"\n",
    "    print(f\"\\nðŸ“¦ Processing trips for year: {year}\")\n",
    "\n",
    "    # Load trip data\n",
    "    trip_df = pd.read_csv(trip_file)\n",
    "    trip_df['starttime'] = pd.to_datetime(trip_df['starttime'], errors='coerce')\n",
    "    trip_df['stoptime'] = pd.to_datetime(trip_df['stoptime'], errors='coerce')\n",
    "\n",
    "    # Filter crash data within time bounds of the trip data\n",
    "    earliest_trip_time = trip_df['starttime'].min()\n",
    "    latest_trip_time = trip_df['starttime'].max()\n",
    "    start_bound = earliest_trip_time - Timedelta(minutes=2)\n",
    "    end_bound = latest_trip_time + Timedelta(minutes=2)\n",
    "\n",
    "    crash_df_chunk = crash_df[\n",
    "        (crash_df['CRASH_DATETIME'] >= start_bound) &\n",
    "        (crash_df['CRASH_DATETIME'] <= end_bound)\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"ðŸš¨ {len(crash_df_chunk)} crash records within trip time range.\")\n",
    "\n",
    "    matched_trip_info = []\n",
    "\n",
    "    for _, crash_row in crash_df_chunk.iterrows():\n",
    "        crash_dt = crash_row['CRASH_DATETIME']\n",
    "        crash_lat = crash_row['LATITUDE']\n",
    "        crash_lng = crash_row['LONGITUDE']\n",
    "        crash_id = crash_row['COLLISION_ID']\n",
    "\n",
    "        # Time window around crash\n",
    "        window_start = crash_dt - timedelta(minutes=max_time_diff)\n",
    "        window_end = crash_dt + timedelta(minutes=max_time_diff)\n",
    "\n",
    "        candidate_trips = trip_df[\n",
    "            ((trip_df['starttime'] >= window_start) & (trip_df['starttime'] <= window_end)) |\n",
    "            ((trip_df['stoptime'] >= window_start) & (trip_df['stoptime'] <= window_end))\n",
    "        ]\n",
    "\n",
    "        for trip_idx, trip_row in candidate_trips.iterrows():\n",
    "            match_type = None\n",
    "\n",
    "            dist_start = haversine(\n",
    "                (trip_row['start station latitude'], trip_row['start station longitude']),\n",
    "                (crash_lat, crash_lng), unit=Unit.METERS\n",
    "            ) if pd.notna(trip_row['start station latitude']) and pd.notna(trip_row['start station longitude']) else float('inf')\n",
    "\n",
    "            dist_end = haversine(\n",
    "                (trip_row['end station latitude'], trip_row['end station longitude']),\n",
    "                (crash_lat, crash_lng), unit=Unit.METERS\n",
    "            ) if pd.notna(trip_row['end station latitude']) and pd.notna(trip_row['end station longitude']) else float('inf')\n",
    "\n",
    "            # Determine match\n",
    "            if dist_start <= max_distance_m and dist_end <= max_distance_m:\n",
    "                match_type = 'both'\n",
    "            elif dist_start <= max_distance_m:\n",
    "                match_type = 'start'\n",
    "            elif dist_end <= max_distance_m:\n",
    "                match_type = 'end'\n",
    "\n",
    "            if match_type:\n",
    "                matched_trip_info.append({\n",
    "                    'trip_idx': trip_idx,\n",
    "                    'crash_id': crash_id,\n",
    "                    'match_type': match_type,\n",
    "                    'dist_start_m': dist_start,\n",
    "                    'dist_end_m': dist_end,\n",
    "                    'CRASH_LATITUDE':crash_lat,\n",
    "                    'CRASH_LONGITUDE':crash_lng,\n",
    "                    'CRASH_DATETIME':crash_dt\n",
    "                })\n",
    "\n",
    "    # Merge matched crash info back to trip_df\n",
    "    trip_df['crash_match_type'] = None\n",
    "    trip_df['matched_crash_id'] = None\n",
    "    trip_df['dist_to_crash_start_m'] = None\n",
    "    trip_df['dist_to_crash_end_m'] = None\n",
    "    trip_df['CRASH_LATITUDE'] = None\n",
    "    trip_df['CRASH_LONGITUDE'] = None\n",
    "    trip_df['CRASH_DATETIME'] = None\n",
    "\n",
    "\n",
    "    matched_df = pd.DataFrame(matched_trip_info)\n",
    "    for _, row in matched_df.iterrows():\n",
    "        trip_df.at[row['trip_idx'], 'crash_match_type'] = row['match_type']\n",
    "        trip_df.at[row['trip_idx'], 'matched_crash_id'] = row['crash_id']\n",
    "        trip_df.at[row['trip_idx'], 'dist_to_crash_start_m'] = row['dist_start_m']\n",
    "        trip_df.at[row['trip_idx'], 'dist_to_crash_end_m'] = row['dist_end_m']\n",
    "        # later used for visualization on map\n",
    "        trip_df.at[row['trip_idx'], 'CRASH_LATITUDE'] = row['CRASH_LATITUDE']\n",
    "        trip_df.at[row['trip_idx'], 'CRASH_LONGITUDE'] = row['CRASH_LONGITUDE']\n",
    "        trip_df.at[row['trip_idx'], 'CRASH_DATETIME'] = row['CRASH_DATETIME']\n",
    "\n",
    "    # Extract matched trips\n",
    "    matched_trips_df = trip_df[trip_df['matched_crash_id'].notna()].copy()\n",
    "    print(f\"âœ… Matched trips found: {len(matched_trips_df)}\")\n",
    "\n",
    "    # Save to results\n",
    "    output_file = results_folder / f\"matched_trips_{year}.csv\"\n",
    "    matched_trips_df.to_csv(output_file, index=False)\n",
    "    print(f\"ðŸ’¾ Saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09d81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88ed4778",
   "metadata": {},
   "source": [
    "# Merge all matched trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8728a6c-cf3b-4830-90c2-acf420928d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading crash data from: results\\matched_trips\\matched_trips_2013.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2014.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2015.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2016.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2017.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2018.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2019.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2020.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2021.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2022.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2023.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2024.csv\n",
      "Loading crash data from: results\\matched_trips\\matched_trips_2025.csv\n",
      "\n",
      "âœ… Combined crash data saved: results\\matched_trips\\matched_crashes_all\\all_years_matched_crashes.csv\n",
      "  tripduration            starttime             stoptime start_station_id  \\\n",
      "0         1354  2013-06-03 17:08:18  2013-06-03 17:30:52              311   \n",
      "1         1525  2013-06-05 17:34:04  2013-06-05 17:59:29              352   \n",
      "2          627  2013-06-05 18:00:34  2013-06-05 18:11:01              352   \n",
      "3         1154  2013-06-06 12:19:17  2013-06-06 12:38:31              152   \n",
      "4          866  2013-06-06 18:33:21  2013-06-06 18:47:47              375   \n",
      "\n",
      "          start station name start station latitude start station longitude  \\\n",
      "0     Norfolk St & Broome St              40.717227              -73.988021   \n",
      "1            W 56 St & 6 Ave              40.763406              -73.977225   \n",
      "2            W 56 St & 6 Ave              40.763406              -73.977225   \n",
      "3      Warren St & Church St               40.71474              -74.009106   \n",
      "4  Mercer St & Bleecker St S              40.726795              -73.996951   \n",
      "\n",
      "  end_station_id    end station name end station latitude  ... Gender ride_id  \\\n",
      "0          532.0     S 5 Pl & S 5 St            40.710451  ...    NaN     NaN   \n",
      "1          116.0     W 17 St & 8 Ave            40.741776  ...    NaN     NaN   \n",
      "2          529.0     W 42 St & 8 Ave             40.75757  ...    NaN     NaN   \n",
      "3          402.0  Broadway & E 22 St            40.740343  ...    NaN     NaN   \n",
      "4          477.0     W 41 St & 8 Ave            40.756405  ...    NaN     NaN   \n",
      "\n",
      "  rideable_type start_station_name end_station_name start_lat start_lng  \\\n",
      "0           NaN                NaN              NaN       NaN       NaN   \n",
      "1           NaN                NaN              NaN       NaN       NaN   \n",
      "2           NaN                NaN              NaN       NaN       NaN   \n",
      "3           NaN                NaN              NaN       NaN       NaN   \n",
      "4           NaN                NaN              NaN       NaN       NaN   \n",
      "\n",
      "  end_lat end_lng bike_type  \n",
      "0     NaN     NaN       NaN  \n",
      "1     NaN     NaN       NaN  \n",
      "2     NaN     NaN       NaN  \n",
      "3     NaN     NaN       NaN  \n",
      "4     NaN     NaN       NaN  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_all_crash_data():\n",
    "    base_dir = \"results\\matched_trips\"\n",
    "    crash_files = [f for f in os.listdir(base_dir) if f.startswith(\"matched_trips_\") and f.endswith(\".csv\")]\n",
    "    crash_files.sort()\n",
    "    \n",
    "    all_crashes = []\n",
    "    for f in crash_files:\n",
    "        file_path = os.path.join(base_dir, f)\n",
    "        print(f\"Loading crash data from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['source_file'] = f  # Optional: keep track of source year/file\n",
    "        all_crashes.append(df)\n",
    "    \n",
    "    if not all_crashes:\n",
    "        print(\"âŒ No crash data files found.\")\n",
    "        return\n",
    "    \n",
    "    combined_crashes = pd.concat(all_crashes, ignore_index=True)\n",
    "    \n",
    "    output_dir = os.path.join(base_dir, \"matched_crashes_all\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, \"all_years_matched_crashes.csv\")\n",
    "    combined_crashes.to_csv(output_file, index=False)\n",
    "    print(f\"\\nâœ… Combined crash data saved: {output_file}\")\n",
    "    print(combined_crashes.head())\n",
    "\n",
    "# Run the merging function\n",
    "merge_all_crash_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2150e77-d698-48b2-af34-b2999483339f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92956de9-5f34-4678-bbbd-3f1cee142f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdb67f-41e3-49ba-ad5f-3b0aae9240cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bda70e-0dc0-48ae-a75a-09daf4c6a684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded4234-5fd7-41cc-a0eb-9224e8df4c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e55eaa1-8d08-4fb6-a878-15996c49d048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crash map saved to: results\\crash_map_by_month_all.html\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59365ae-1d8c-4880-9c10-3686f98c6e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
