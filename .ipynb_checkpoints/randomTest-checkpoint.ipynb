{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796311a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: convert only if not already datetime\n",
    "def safe_convert_to_datetime(series):\n",
    "    if not pd.api.types.is_datetime64_any_dtype(series):\n",
    "        # Clean multiple spaces, strip whitespace\n",
    "        series = series.astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "        return pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n",
    "    return series\n",
    "\n",
    "# === Step 2: Normalize different schemas to common format ===\n",
    "def normalize_columns(df):\n",
    "    col_map = {\n",
    "        # Time columns\n",
    "        'starttime': 'starttime',\n",
    "        'Start Time': 'starttime',\n",
    "        'started_at': 'starttime',\n",
    "\n",
    "        'stoptime': 'stoptime',\n",
    "        'Stop Time': 'stoptime',\n",
    "        'ended_at': 'stoptime',\n",
    "\n",
    "        # Optional: unify naming for trip duration or other fields\n",
    "        'tripduration': 'tripduration',\n",
    "        'Trip Duration': 'tripduration',\n",
    "\n",
    "        # User type\n",
    "        'usertype': 'usertype',\n",
    "        'User Type': 'usertype',\n",
    "        'member_casual': 'usertype',\n",
    "\n",
    "        # Start station ID\n",
    "        'start station id': 'start_station_id',\n",
    "        'Start Station ID': 'start_station_id',\n",
    "        'start_station_id': 'start_station_id',\n",
    "\n",
    "        # End station ID\n",
    "        'end station id': 'end_station_id',\n",
    "        'End Station ID': 'end_station_id',\n",
    "        'end_station_id': 'end_station_id',\n",
    "\n",
    "        # Optional: bike id\n",
    "        'bikeid': 'bike_id',\n",
    "        'Bike ID': 'bike_id',\n",
    "\n",
    "        # Ride ID (Divvy)\n",
    "        'ride_id': 'ride_id',\n",
    "        'rideable_type': 'rideable_type'\n",
    "    }\n",
    "\n",
    "    # Rename columns based on known mappings\n",
    "    df = df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf713c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Found 20 CSV files in extracted_tripdata\\2015\n",
      "  📖 Reading: extracted_tripdata\\2015\\201501-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201502-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201503-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201504-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201505-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201506-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201507-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201507-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201508-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201508-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201509-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201509-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201510-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201510-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201511-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\201512-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\JC-201509-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\JC-201510-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\JC-201511-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2015\\JC-201512-citibike-tripdata.csv\n",
      "Original rows: 9990852\n",
      "  ✅ Merged CSV saved to: merged_tripdata3\\2015_merged.csv\n",
      "\n",
      "📁 Found 32 CSV files in extracted_tripdata\\2016\n",
      "  📖 Reading: extracted_tripdata\\2016\\201601-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201602-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201603-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201604-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201604-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201605-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201605-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201606-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201606-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201607-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201607-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201608-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201608-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201609-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201609-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201610-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201610-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201611-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201611-citibike-tripdata_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\201612-citibike-tripdata_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201604-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201605-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201606-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201607-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201608-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201609-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-20161-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201610-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201611-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-201612-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-20162-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2016\\JC-20163-citibike-tripdata.csv\n",
      "Original rows: 14093239\n",
      "  ✅ Merged CSV saved to: merged_tripdata3\\2016_merged.csv\n",
      "\n",
      "📁 Found 32 CSV files in extracted_tripdata\\2017\n",
      "  📖 Reading: extracted_tripdata\\2017\\201701-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201702-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201703-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201704-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201704-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201705-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201705-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201706-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201706-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201707-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201707-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201708-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201708-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201709-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201709-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201710-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201710-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201711-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201711-citibike-tripdata.csv_2.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\201712-citibike-tripdata.csv_1.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201701-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201702-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201703-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201704-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201705-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201706-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201707-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201708 citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201709-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201710-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201711-citibike-tripdata.csv\n",
      "  📖 Reading: extracted_tripdata\\2017\\JC-201712-citibike-tripdata.csv\n",
      "Original rows: 16659585\n",
      "  ✅ Merged CSV saved to: merged_tripdata3\\2017_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files(input_dir, output_file):\n",
    "    all_csv_files = [\n",
    "        os.path.join(input_dir, f)\n",
    "        for f in os.listdir(input_dir)\n",
    "        if f.lower().endswith('.csv')\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n📁 Found {len(all_csv_files)} CSV files in {input_dir}\")\n",
    "\n",
    "    df_list = []\n",
    "    for file in all_csv_files:\n",
    "        print(f\"  📖 Reading: {file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Normalize columns\n",
    "            df = normalize_columns(df)\n",
    "            # Apply safe conversion\n",
    "            df['starttime'] = safe_convert_to_datetime(df['starttime'])\n",
    "            df['stoptime'] = safe_convert_to_datetime(df['stoptime'])\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error reading {file}: {e}\")\n",
    "\n",
    "    if df_list:\n",
    "        \n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "        #cleaning\n",
    "        print(f\"Original rows: {len(merged_df)}\")\n",
    "        # Remove duplicates (based on all columns)\n",
    "        merged_df = merged_df.drop_duplicates()\n",
    "        merged_df = merged_df.sort_values(by='starttime')\n",
    "\n",
    "    \t#print(f\"Rows after removing duplicates: {len(merged_df)}\")\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"  ✅ Merged CSV saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"  ⚠️ No valid CSV files to merge.\")\n",
    "\n",
    "# === CONFIG ===\n",
    "input_root = r\"extracted_tripdata\"              # Parent folder with year subfolders\n",
    "output_root = r\"merged_tripdata3\"                # Where to save merged CSVs\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "selected_years = ['2015', '2016', '2017']\n",
    "\n",
    "for year in selected_years:\n",
    "# Loop over all subfolders (years)\n",
    "#for folder_name in os.listdir(input_root):\n",
    "    folder_path = os.path.join(input_root, year)\n",
    "    \n",
    "    if os.path.isdir(folder_path) and year.isdigit():\n",
    "        year = year\n",
    "        output_file = os.path.join(output_root, f\"{year}_merged.csv\")\n",
    "        merge_csv_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e41b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4c8ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year   Folder 1   Folder 2   Match?\n",
      "2013   17         17         True\n",
      "2014   12         12         True\n",
      "2015   20         20         True\n",
      "2016   32         32         True\n",
      "2017   32         32         True\n",
      "2018   45         45         True\n",
      "2019   38         38         True\n",
      "2020   39         12         False\n",
      "2021   48         12         False\n",
      "2022   48         12         False\n",
      "2023   52         12         False\n",
      "2024   62         62         True\n",
      "2025   50         50         True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_csv_files_in_year_folder(base_dir, year_range=range(2013, 2026)):\n",
    "    year_counts = {}\n",
    "    for year in map(str, year_range):\n",
    "        year_path = os.path.join(base_dir, year)\n",
    "        if not os.path.exists(year_path):\n",
    "            year_counts[year] = None  # Year folder missing\n",
    "            continue\n",
    "        count = 0\n",
    "        for root, _, files in os.walk(year_path):\n",
    "            if '__MACOSX' in root:\n",
    "                continue\n",
    "            count += sum(1 for file in files if file.endswith(\".csv\"))\n",
    "        year_counts[year] = count\n",
    "    return year_counts\n",
    "\n",
    "# Set your folder paths\n",
    "folder1 = r\"C:\\Users\\yanhu\\Documents\\Python_Notebook\\AXA_task\\extracted_tripdata4\"\n",
    "folder2 = r\"H:\\Projekte\\others\\extracted_tripdata\"\n",
    "\n",
    "# Count CSVs\n",
    "counts1 = count_csv_files_in_year_folder(folder1)\n",
    "counts2 = count_csv_files_in_year_folder(folder2)\n",
    "\n",
    "# Print comparison\n",
    "print(f\"{'Year':<6} {'Folder 1':<10} {'Folder 2':<10} {'Match?':<6}\")\n",
    "for year in sorted(set(counts1.keys()) | set(counts2.keys())):\n",
    "    c1 = counts1.get(year)\n",
    "    c2 = counts2.get(year)\n",
    "    match = (c1 == c2) if (c1 is not None and c2 is not None) else \"N/A\"\n",
    "    print(f\"{year:<6} {str(c1):<10} {str(c2):<10} {match}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f6568e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04003bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
